{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Classificador Automático de Sentimento\n",
    "\n",
    "Você foi contratado por uma empresa parar analisar como os clientes estão reagindo a um determinado produto no Twitter. A empresa deseja que você crie um programa que irá analisar as mensagens disponíveis e classificará como \"relevante\" ou \"irrelevante\". Com isso ela deseja que mensagens negativas, que denigrem o nome do produto, ou que mereçam destaque, disparem um foco de atenção da área de marketing.<br /><br />\n",
    "Como aluno de Ciência dos Dados, você lembrou do Teorema de Bayes, mais especificamente do Classificador Naive-Bayes, que é largamente utilizado em filtros anti-spam de e-mails. O classificador permite calcular qual a probabilidade de uma mensagem ser relevante dadas as palavras em seu conteúdo.<br /><br />\n",
    "Para realizar o MVP (*minimum viable product*) do projeto, você precisa implementar uma versão do classificador que \"aprende\" o que é relevante com uma base de treinamento e compara a performance dos resultados com uma base de testes.<br /><br />\n",
    "Após validado, o seu protótipo poderá também capturar e classificar automaticamente as mensagens da plataforma.\n",
    "\n",
    "## Informações do Projeto\n",
    "\n",
    "Prazo: 13/Set até às 23:59.<br />\n",
    "Grupo: 1 ou 2 pessoas.<br /><br />\n",
    "Entregáveis via GitHub: \n",
    "* Arquivo notebook com o código do classificador, seguindo as orientações abaixo.\n",
    "* Arquivo Excel com as bases de treinamento e teste totalmente classificado.\n",
    "\n",
    "**NÃO disponibilizar o arquivo com os *access keys/tokens* do Twitter.**\n",
    "\n",
    "\n",
    "### Check 3: \n",
    "\n",
    "Até o dia 06 de Setembro às 23:59, o notebook e o xlsx devem estar no Github com as seguintes evidências: \n",
    "    * Conta no twitter criada.\n",
    "    * Produto escolhido.\n",
    "    * Arquivo Excel contendo a base de treinamento e teste já classificado.\n",
    "    \n",
    "\n",
    "Sugestão de leitura:<br />\n",
    "http://docs.tweepy.org/en/v3.5.0/index.html<br />\n",
    "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Preparando o ambiente\n",
    "\n",
    "Instalando a biblioteca *tweepy* para realizar a conexão com o Twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as Bibliotecas que serão utilizadas. Esteja livre para adicionar outras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticando no  Twitter\n",
    "\n",
    "Para realizar a captura dos dados é necessário ter uma conta cadastrada no twitter:\n",
    "\n",
    "* Conta: @bjsmjil\n",
    "\n",
    "\n",
    "1. Caso ainda não tenha uma: https://twitter.com/signup\n",
    "1. Depois é necessário registrar um app para usar a biblioteca: https://apps.twitter.com/\n",
    "1. Dentro do registro do App, na aba Keys and Access Tokens, anotar os seguintes campos:\n",
    "    1. Consumer Key (API Key) = JQNEcKGAjiyDLocQtNAMMVKeI\n",
    "    1. Consumer Secret (API Secret) = 7ERqHyCg6saIEYhOPcjrDHBH6loH54FNDOCUyYYIIytaeoLRnM\n",
    "\n",
    "1. Mais abaixo, gere um Token e anote também:\n",
    "    1. Access Token = 903304086061187073-oUyvgEs4T0bxP8HQmks7OfqJs3z0H1F\n",
    "    1. Access Token Secret = iJp2XXkfZhE2wz5p35IFR4ieMkjdc4lzstzLs40QydFPz\n",
    "\n",
    "    \n",
    "1. Preencha os valores no arquivo \"auth.pass\"\n",
    "\n",
    "**ATENÇÃO**: Nunca divulgue os dados desse arquivo online (GitHub, etc). Ele contém as chaves necessárias para realizar as operações no twitter de forma automática e portanto é equivalente a ser \"hackeado\". De posse desses dados, pessoas mal intencionadas podem fazer todas as operações manuais (tweetar, seguir, bloquear/desbloquear, listar os seguidores, etc). Para efeito do projeto, esse arquivo não precisa ser entregue!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'auth.pass'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-71cd346a41cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#leitura do arquivo no formato JSON\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'auth.pass'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'auth.pass'"
     ]
    }
   ],
   "source": [
    "#Dados de autenticação do twitter:\n",
    "\n",
    "#Coloque aqui o identificador da conta no twitter: @fulano\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. Não modificar\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Coletando Dados\n",
    "\n",
    "Agora vamos coletar os dados. Tenha em mente que dependendo do produto escolhido, não haverá uma quantidade significativa de mensagens, ou ainda poder haver muitos retweets.<br /><br /> \n",
    "Configurando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Produto escolhido:\n",
    "produto = 'Netshoes'\n",
    "\n",
    "#Quantidade mínima de mensagens capturadas:\n",
    "n = 500\n",
    "#Quantidade mínima de mensagens para a base de treinamento:\n",
    "t = 300\n",
    "\n",
    "#Filtro de língua, escolha uma na tabela ISO 639-1.\n",
    "lang = 'pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturando os dados do twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cria um objeto para a captura\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#Inicia a captura, para mais detalhes: ver a documentação do tweepy\n",
    "i = 1\n",
    "msgs = []\n",
    "for msg in tweepy.Cursor(api.search, q=produto, lang=lang).items():    \n",
    "    msgs.append(msg.text.lower())\n",
    "    i += 1\n",
    "    if i > n:\n",
    "        break\n",
    "\n",
    "#Embaralhando as mensagens para reduzir um possível viés\n",
    "shuffle(msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando os dados em uma planilha Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Verifica se o arquivo não existe para não substituir um conjunto pronto\n",
    "if not os.path.isfile('./{0}.xlsx'.format(produto)):\n",
    "    \n",
    "    #Abre o arquivo para escrita\n",
    "    writer = pd.ExcelWriter('{0}.xlsx'.format(produto))\n",
    "\n",
    "    #divide o conjunto de mensagens em duas planilhas\n",
    "    dft = pd.DataFrame({'Treinamento' : pd.Series(msgs[:t])})\n",
    "    dft.to_excel(excel_writer = writer, sheet_name = 'Treinamento', index = False)\n",
    "\n",
    "    dfc = pd.DataFrame({'Teste' : pd.Series(msgs[t:])})\n",
    "    dfc.to_excel(excel_writer = writer, sheet_name = 'Teste', index = False)\n",
    "\n",
    "    #fecha o arquivo\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Classificando as Mensagens\n",
    "\n",
    "Agora você deve abrir o arquivo Excel com as mensagens capturadas e classificar na Coluna B se a mensagem é relevante ou não.<br /> \n",
    "Não se esqueça de colocar um nome para a coluna na célula **B1**.<br /><br />\n",
    "Fazer o mesmo na planilha de Controle.\n",
    "\n",
    "___\n",
    "## Montando o Classificador Naive-Bayes\n",
    "\n",
    "Com a base de treinamento montada, comece a desenvolver o classificador. Escreva o seu código abaixo:\n",
    "\n",
    "Opcionalmente: \n",
    "* Limpar as mensagens removendo os caracteres: enter, :, \", ', (, ), etc. Não remover emojis.<br />\n",
    "* Corrigir separação de espaços entre palavras e/ou emojis.\n",
    "* Propor outras limpezas/transformações que não afetem a qualidade da informação.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# função que remove pontuações\n",
    "import string\n",
    "def tira_pontuacao(texto):\n",
    "    for punctuation in string.punctuation:\n",
    "        texto = texto.replace(punctuation, '')\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dados = pd.read_excel('Netshoes.xlsx')\n",
    "relevantes_treinamento=[]\n",
    "irrelevantes_treinamento=[]\n",
    "dados.rename(columns={\n",
    "                 'Unnamed: 1': 'Relevância'}, inplace=True)\n",
    "dados[\"Treinamento\"] = dados['Treinamento'].apply(tira_pontuacao)\n",
    "\n",
    "# separando quanto a relevância\n",
    "relevantes = dados[(dados['Relevância'] == 'S')]\n",
    "irrelevantes = dados[(dados['Relevância'] == 'A')]\n",
    "\n",
    "# Transformando siglas\n",
    "dados.loc[(dados['Relevância']=='S'),'Relevância'] == 'Relevante'\n",
    "dados.loc[(dados['Relevância']=='A'),'Relevância'] == 'Irelevante'\n",
    "\n",
    "#limpando twitts\n",
    "\n",
    "for i in range(len(dados)):\n",
    "    linha = dados.iloc[i]['Treinamento']\n",
    "    linha = linha.split()\n",
    "    for j in range(len(linha)):\n",
    "        linha[j]=linha[j].lower()\n",
    "        linha[j]=linha[j].strip()\n",
    "        linha[j] = linha[j].replace(\".\", \"\")\n",
    "    dados.iloc[i]['Treinamento'] = linha\n",
    "\n",
    "\n",
    "# selecionando palavras dos relevantes\n",
    "for i in range(len(relevantes)):\n",
    "    linha = relevantes.iloc[i]['Treinamento']\n",
    "    linha = linha.split()\n",
    "    for j in range(len(linha)):\n",
    "        linha[j]=linha[j].lower()\n",
    "        linha[j]=linha[j].strip()\n",
    "        linha[j] = linha[j].replace(\".\", \"\")\n",
    "        relevantes_treinamento.append(linha[j])\n",
    "    relevantes.iloc[i]['Treinamento'] = linha\n",
    "    \n",
    "# selecionando palavras dos irrelevantes\n",
    "for i in range(len(irrelevantes)):\n",
    "    linha = irrelevantes.iloc[i]['Treinamento']\n",
    "    linha = linha.split()\n",
    "    for j in range(len(linha)):\n",
    "        linha[j]=linha[j].lower()\n",
    "        linha[j]=linha[j].strip()\n",
    "        linha[j] = linha[j].replace(\".\", \"\")\n",
    "        irrelevantes_treinamento.append(linha[j])\n",
    "    irrelevantes.iloc[i]['Treinamento'] = linha\n",
    "    \n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Verificando a performance\n",
    "\n",
    "Agora você deve testar o seu Classificador com a base de Testes.<br /><br /> \n",
    "\n",
    "Você deve extrair as seguintes medidas:\n",
    "* Porcentagem de positivos falsos (marcados como relevante mas não são relevantes)\n",
    "* Porcentagem de positivos verdadeiros (marcado como relevante e são relevantes)\n",
    "* Porcentagem de negativos verdadeiros (marcado como não relevante e não são relevantes)\n",
    "* Porcentagem de negativos falsos (marcado como não relevante e são relevantes)\n",
    "\n",
    "Opcionalmente:\n",
    "* Criar categorias intermediárias de relevância baseado na diferença de probabilidades. Exemplo: muito relevante, relevante, neutro, irrelevante e muito irrelevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# planilha teste\n",
    "\n",
    "teste = pd.read_excel('Netshoes.xlsx', sheetname='Teste')\n",
    "\n",
    "teste.rename(columns={\n",
    "                 'Unnamed: 1': 'Relevância'}, inplace=True)\n",
    "teste[\"Teste\"] = teste['Teste'].apply(tira_pontuacao)\n",
    "#limpando twitts\n",
    "for i in range(len(teste)):\n",
    "    linha = teste.iloc[i]['Teste']\n",
    "    linha = linha.split()\n",
    "    if 'netshoes' in linha:\n",
    "        linha.remove('netshoes')\n",
    "    if 'de' in linha:\n",
    "        linha.remove('de')\n",
    "    if 'o' in linha:\n",
    "        linha.remove('o')\n",
    "    if 'que' in linha:\n",
    "        linha.remove('que')\n",
    "    if 'em' in linha:\n",
    "        linha.remove('em')\n",
    "    if 'tem' in linha:\n",
    "        linha.remove('tem')\n",
    "    if 'no' in linha:\n",
    "        linha.remove('no')\n",
    "    if 'do' in linha:\n",
    "        linha.remove('do')\n",
    "    if 'um' in linha:\n",
    "        linha.remove('um')\n",
    "    if 'para' in linha:\n",
    "        linha.remove('para')\n",
    "    if 'é' in linha:    \n",
    "        linha.remove('é')\n",
    "    if 'e' in linha:    \n",
    "        linha.remove('e')\n",
    "    if 'a' in linha:    \n",
    "        linha.remove('a')\n",
    "    if 'pra' in linha:\n",
    "        linha.remove('pra')\n",
    "\n",
    "        \n",
    "    for j in range(len(linha)):\n",
    "        linha[j]=linha[j].lower()\n",
    "        linha[j]=linha[j].strip()\n",
    "        linha[j] = linha[j].replace(\".\", \"\")\n",
    "    teste.iloc[i]['Teste'] = linha\n",
    "\n",
    "\n",
    "# series relevantes\n",
    "prob_wr = pd.Series(relevantes_treinamento).value_counts().to_dict()\n",
    "\n",
    "# series irrelevantes\n",
    "prob_wir = pd.Series(irrelevantes_treinamento).value_counts().to_dict()\n",
    "\n",
    "# probabilidade da sentença ser relevante\n",
    "todas_palavras = len(prob_wr)+len(prob_wir)\n",
    "prob_relevante = len(prob_wr)/todas_palavras\n",
    "prob_irrelevante = len(prob_wir)/todas_palavras    \n",
    "\n",
    "\n",
    "# cálculo de probabilidade dos tweets ser relevante\n",
    "prob_twitt_r=[]\n",
    "for i in range(len(teste)):\n",
    "    twitt= teste.iloc[i]['Teste']\n",
    "    prob_twitt=1\n",
    "    y=1\n",
    "    for j in range(len(twitt)):\n",
    "        for word in prob_wr:\n",
    "            if twitt[j]==word:\n",
    "                y=prob_wr[word]+1\n",
    "                break\n",
    "        prob_word=y/(len(prob_wr)+todas_palavras)\n",
    "        prob_twitt*=prob_word\n",
    "    prob_twitt_r.append(prob_twitt)\n",
    "\n",
    "    \n",
    "\n",
    "#cálculo de probabilidade dos tweets ser irrelevante    \n",
    "prob_twitt_ir=[]\n",
    "for i in range(len(teste)):\n",
    "    twitt= teste.iloc[i]['Teste']\n",
    "    prob_twitt=1\n",
    "    y=1\n",
    "    for j in range(len(twitt)):\n",
    "        for word in prob_wir:\n",
    "            if twitt[j]==word:\n",
    "                y=prob_wir[word]+1\n",
    "                break\n",
    "        prob_word=y/(len(prob_wir)+todas_palavras)\n",
    "        prob_twitt*=prob_word   \n",
    "    prob_twitt_ir.append(prob_twitt)\n",
    "        \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tomada de decisão\n",
    "\n",
    "for index, row in teste.iterrows():\n",
    "    i = teste.index.get_loc(index)\n",
    "    if prob_twitt_r[i]*prob_relevante>prob_twitt_ir[i]*prob_irrelevante:\n",
    "        teste.loc[index, 'Classificador'] = 'Relevante'\n",
    "\n",
    "\n",
    "    if prob_twitt_r[i]*prob_relevante<prob_twitt_ir[i]*prob_irrelevante:\n",
    "        teste.loc[index, 'Classificador'] = 'Irrelevante'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Comparativo = pd.crosstab(teste['Relevância'],teste['Classificador'])\n",
    "Comparativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PPF = (32/(17+32))*100\n",
    "PPV = (17/(17+32))*100\n",
    "PNV = (133/(18+133))*100\n",
    "PNF = (18/(18+133))*100\n",
    "\n",
    "print('Porcentagem de positivos falsos ',PPF,'\\nPorcentagem de positivos verdadeiros',PPV,'\\nPorcentagem de negativos verdadeiros',PNV,'\\nPorcentagem de negativos falsos',PNF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Concluindo\n",
    "\n",
    "Escreva aqui a sua conclusão.<br /> \n",
    "Faça um comparativo qualitativo sobre as medidas obtidas.<br />\n",
    "Explique como são tratadas as mensagens com dupla negação e sarcasmo.<br />\n",
    "Proponha um plano de expansão. Por que eles devem continuar financiando o seu projeto?<br />\n",
    "\n",
    "Opcionalmente: \n",
    "* Discorrer por que não posso alimentar minha base de Treinamento automaticamente usando o próprio classificador, aplicado a novos tweets.\n",
    "* Propor diferentes cenários de uso para o classificador Naive-Bayes. Cenários sem intersecção com este projeto.\n",
    "* Sugerir e explicar melhorias reais no classificador com indicações concretas de como implementar (não é preciso codificar, mas indicar como fazer e material de pesquisa sobre o assunto).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "       \n",
    "       Após a coleta de dados feita pelo código presente neste documento, foi realizada uma classificação manual para determinar os tweets que são considerados relevantes ou não para alimentar o classificador automático de forma que ele aprenda as palavras que mais se adequam em cada tipo automaticamente.\n",
    "        O classificador apresenta uma certa ambiguidade na forma como apresenta os dados pois por um lado classifica mal os tweets relevantes afinal a porcentagem de tweets classificados como relevantes e que não são é muito alta, assim como os que são relevantes têm uma porcentagem baixa, e por outro acaba classificando bem os tweets que não são relevantes, tendo uma alta porcentagem de acerto.\n",
    "        O sistema do classificador não é preparado para interpretar sarcasmo ou dupla negação pois o método Naive-Bayes vizualiza cada palavra da senteça como uma probabilidade para rotular a frase como relevante ou não, sem analisar expressões ou sentimentos.\n",
    "        Para além do projeto de classificar como sendo relevante ou não cada tweet o programa pode ser aperfeiçoado para apresentar uma classificação de humor do cliente, assim como definir se entre os tweets relevantes existe algum problema que é tratado de forma recorrente e isso exigirá um método que analise não somente palavras de uma frase mas também expressões. \n",
    "        O projeto é um indicador muito bom para relatar a irrelevância de uma frase por mesmo apresentando falhas na forma como avalia a relevância da mesma. Dado isto é satisfatória a forma como este trabalho opera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
